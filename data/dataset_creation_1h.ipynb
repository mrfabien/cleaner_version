{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook to extract the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "operating_system ='curnagl'\n",
    "if operating_system == 'win':\n",
    "    os.chdir('C:/Users/fabau/OneDrive/Documents/GitHub/master-project-cleaned/')\n",
    "    path = f'C:/Users/fabau/OneDrive/Documents/GitHub/master-project/'\n",
    "elif operating_system == 'curnagl':\n",
    "    os.chdir('/work/FAC/FGSE/IDYST/tbeucler/default/fabien/repos/cleaner_version/')\n",
    "    path = f'/work/FAC/FGSE/IDYST/tbeucler/default/fabien/repos/cleaner_version/'\n",
    "else:\n",
    "    os.chdir('/Users/fabienaugsburger/Documents/GitHub/master-project-cleaned/')\n",
    "    path = f'/Users/fabienaugsburger/Documents/GitHub/master-project-cleaned/'\n",
    "\n",
    "# Add the path to your custom library\n",
    "'''if operating_system == 'curnagl':\n",
    "    custom_library_path = os.path.abspath(f'{path}/cleaner_version/util/processing/')\n",
    "else:'''\n",
    "custom_library_path = os.path.abspath(f'{path}/util/processing/')\n",
    "sys.path.append(custom_library_path)\n",
    "\n",
    "import extraction_squares\n",
    "\n",
    "levels = pd.read_csv(f'{path}data/levels.csv')\n",
    "variables = (f'{path}data/variable_list_SL.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/work/FAC/FGSE/IDYST/tbeucler/default/fabien/repos/cleaner_version\n"
     ]
    }
   ],
   "source": [
    "# get current working directory\n",
    "cwd = os.getcwd()\n",
    "print(cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of the configuration file\n",
    "\n",
    "config_file = f'{path}/data/config_35_var.txt'\n",
    "start_year = 1990\n",
    "end_year = 2021\n",
    "variable_to_exclude = ['specific_rain_water_content', 'relative_humidity', 'specific_humidity', 'vertical_velocity']\n",
    "years_to_exclude = [2003, 2012, 2018]\n",
    "full_pressure = levels['levels'].values\n",
    "\n",
    "extraction_squares.generate_config(variables, config_file, start_year, end_year, variable_to_exclude, years_to_exclude, full_pressure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from extraction_squares import filter_rows_exclude, filter_rows, read_column_values\n",
    "\n",
    "def filtering_storms(\n",
    "    variables_csv,\n",
    "    timestep,\n",
    "    path,\n",
    "    choosen_directory,\n",
    "    levels,\n",
    "    filter_type='EU',\n",
    "    continuous_EU=False,\n",
    "    continuous_non_EU=True,\n",
    "    all_details=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Generalized filtering function for both EU and non-EU storms with options for continuous or non-continuous filtering.\n",
    "\n",
    "    Args:\n",
    "        variables_csv (str): Path to the variables CSV file.\n",
    "        timestep (str): Timestep for the datasets.\n",
    "        path (str): Base path for datasets and outputs.\n",
    "        choosen_directory (str): Directory to save the output datasets.\n",
    "        levels (pd.DataFrame): Dataframe containing levels information.\n",
    "        filter_type (str): Either 'EU' or 'non_EU' to indicate the type of filtering.\n",
    "        continuous_EU (bool): For EU filtering, if True keeps the first continuous EU steps, otherwise keeps all EU steps. Default False.\n",
    "        continuous_non_EU (bool): For non-EU filtering, if True keeps steps up to landfall-1, otherwise keeps all non-EU steps. Default True.\n",
    "        all_details (bool): Print detailed logs. Default False.\n",
    "    \"\"\"\n",
    "    levels = levels['levels'].to_list()\n",
    "\n",
    "    # Read variables\n",
    "    variables = pd.read_csv(variables_csv)['variables']\n",
    "    storms = [f\"{i}\" for i in range(1, 97)]\n",
    "    stats = [\"max\", \"mean\", \"min\", \"std\"]\n",
    "\n",
    "    base_dir_csv1 = f\"{path}data/datasets_{timestep}\"\n",
    "    base_dir_csv2 = f\"{path}pre_processing/tracks/ALL_TRACKS/tracks_{timestep}_EU\"\n",
    "\n",
    "    output_base_dir = f\"{path}{choosen_directory}datasets_{timestep}_{filter_type}\"\n",
    "    os.makedirs(output_base_dir, exist_ok=True)\n",
    "\n",
    "    print(f\"Taking data from {base_dir_csv1}\")\n",
    "    print(f\"With condition based on {base_dir_csv2}\")\n",
    "\n",
    "    for variable in variables:\n",
    "        os.makedirs(os.path.join(output_base_dir, variable), exist_ok=True)\n",
    "        for storm in storms:\n",
    "            os.makedirs(os.path.join(output_base_dir, variable, f\"storm_{storm}\"), exist_ok=True)\n",
    "            for level in levels:\n",
    "                for stat in stats:\n",
    "                    csv_file1 = os.path.join(base_dir_csv1, variable, f\"storm_{storm}\", f\"{stat}_{storm}_{level}.csv\")\n",
    "                    csv_file2 = os.path.join(base_dir_csv2, f\"storm_{storm}.csv\")\n",
    "                    output_file = os.path.join(output_base_dir, variable, f\"storm_{storm}\", f\"{stat}_{storm}_{level}.csv\")\n",
    "\n",
    "                    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "\n",
    "                    if os.path.exists(csv_file1) and os.path.exists(csv_file2):\n",
    "                        filter_values = read_column_values(csv_file2, 'step')\n",
    "\n",
    "                        # EU-specific filtering\n",
    "                        if filter_type == 'EU':\n",
    "                            if continuous_EU:\n",
    "                                # Keep only the first continuous block of EU steps\n",
    "                                first_non_EU_idx = next((i for i, step in enumerate(filter_values) if step < filter_values[0]), len(filter_values))\n",
    "                                filter_values = filter_values[:first_non_EU_idx]\n",
    "\n",
    "                        # Non-EU-specific filtering\n",
    "                        elif filter_type == 'non_EU':\n",
    "                            if continuous_non_EU:\n",
    "                                # Keep steps up to landfall-1\n",
    "                                if len(filter_values) > 1:\n",
    "                                    filter_values = filter_values[:-1]  # Exclude last step (landfall)\n",
    "                                else:\n",
    "                                    filter_values = []  # Handle edge case for storms with no valid landfall\n",
    "\n",
    "                        # Apply filtering logic\n",
    "                        if filter_type == 'EU':\n",
    "                            filter_rows(csv_file1, output_file, '', filter_values)\n",
    "                        elif filter_type == 'non_EU':\n",
    "                            filter_rows_exclude(csv_file1, output_file, '', filter_values)\n",
    "\n",
    "                        if all_details:\n",
    "                            print(f\"Filtered rows for {variable}, {storm}, level {level}, stat {stat} written to {output_file}\")\n",
    "                    else:\n",
    "                        if all_details:\n",
    "                            print(f\"Skipped {variable}, {storm}, level {level}, stat {stat} due to missing files\")\n",
    "        print(f\"Finished filtering {variable}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filtering_storms(\n",
    "    variables_csv,\n",
    "    timestep,\n",
    "    path,\n",
    "    choosen_directory,\n",
    "    levels,\n",
    "    filter_type='EU',\n",
    "    continuous_EU=False,\n",
    "    continuous_non_EU=True,\n",
    "    all_details=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Generalized filtering function for both EU and non-EU storms with options for continuous or non-continuous filtering.\n",
    "\n",
    "    Args:\n",
    "        variables_csv (str): Path to the variables CSV file.\n",
    "        timestep (str): Timestep for the datasets.\n",
    "        path (str): Base path for datasets and outputs.\n",
    "        choosen_directory (str): Directory to save the output datasets.\n",
    "        levels (pd.DataFrame): Dataframe containing levels information.\n",
    "        filter_type (str): Either 'EU' or 'non_EU' to indicate the type of filtering.\n",
    "        continuous_EU (bool): For EU filtering, if True keeps the first continuous EU steps, otherwise keeps all EU steps. Default False.\n",
    "        continuous_non_EU (bool): For non-EU filtering, if True keeps continuous steps up to a gap, otherwise keeps all non-EU steps. Default True.\n",
    "        all_details (bool): Print detailed logs. Default False.\n",
    "    \"\"\"\n",
    "    levels = levels['levels'].to_list()\n",
    "\n",
    "    # Read variables\n",
    "    variables = pd.read_csv(variables_csv)['variables']\n",
    "    storms = [f\"{i}\" for i in range(1, 97)]\n",
    "    stats = [\"max\", \"mean\", \"min\", \"std\"]\n",
    "\n",
    "    base_dir_csv1 = f\"{path}data/datasets_{timestep}\"\n",
    "    base_dir_csv2 = f\"{path}pre_processing/tracks/ALL_TRACKS/tracks_{timestep}_EU\"\n",
    "\n",
    "    output_base_dir = f\"{path}{choosen_directory}datasets_{timestep}_{filter_type}\"\n",
    "    os.makedirs(output_base_dir, exist_ok=True)\n",
    "\n",
    "    print(f\"Taking data from {base_dir_csv1}\")\n",
    "    print(f\"With condition based on {base_dir_csv2}\")\n",
    "\n",
    "    for variable in variables:\n",
    "        os.makedirs(os.path.join(output_base_dir, variable), exist_ok=True)\n",
    "        for storm in storms:\n",
    "            os.makedirs(os.path.join(output_base_dir, variable, f\"storm_{storm}\"), exist_ok=True)\n",
    "            for level in levels:\n",
    "                for stat in stats:\n",
    "                    csv_file1 = os.path.join(base_dir_csv1, variable, f\"storm_{storm}\", f\"{stat}_{storm}_{level}.csv\")\n",
    "                    csv_file2 = os.path.join(base_dir_csv2, f\"storm_{storm}.csv\")\n",
    "                    output_file = os.path.join(output_base_dir, variable, f\"storm_{storm}\", f\"{stat}_{storm}_{level}.csv\")\n",
    "\n",
    "                    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "\n",
    "                    if os.path.exists(csv_file1) and os.path.exists(csv_file2):\n",
    "                        filter_values = read_column_values(csv_file2, 'step')\n",
    "                        # Ensure steps are numeric\n",
    "                        filter_values = [int(value) for value in filter_values]  # Convert to integers\n",
    "\n",
    "                        # EU-specific filtering\n",
    "                        if filter_type == 'EU':\n",
    "                            if continuous_EU:\n",
    "                                # Keep only the first continuous block of EU steps\n",
    "                                first_non_EU_idx = next((i for i, step in enumerate(filter_values) if step < filter_values[0]), len(filter_values))\n",
    "                                filter_values = filter_values[:first_non_EU_idx]\n",
    "\n",
    "                        # Non-EU-specific filtering\n",
    "                        elif filter_type == 'non_EU':\n",
    "                            if continuous_non_EU:\n",
    "                                # Identify the first discontinuity in steps\n",
    "                                continuous_values = []\n",
    "                                for i in range(len(filter_values) - 1):\n",
    "                                    continuous_values.append(filter_values[i])\n",
    "                                    if filter_values[i + 1] - filter_values[i] > 1:  # Check for a gap\n",
    "                                        break\n",
    "                                filter_values = continuous_values\n",
    "\n",
    "                        # Apply filtering logic\n",
    "                        if filter_type == 'EU':\n",
    "                            filter_rows(csv_file1, output_file, '', filter_values)\n",
    "                        elif filter_type == 'non_EU':\n",
    "                            filter_rows_exclude(csv_file1, output_file, '', filter_values)\n",
    "\n",
    "                        if all_details:\n",
    "                            print(f\"Filtered rows for {variable}, {storm}, level {level}, stat {stat} written to {output_file}\")\n",
    "                    else:\n",
    "                        if all_details:\n",
    "                            print(f\"Skipped {variable}, {storm}, level {level}, stat {stat} due to missing files\")\n",
    "        print(f\"Finished filtering {variable}\")\n",
    "        print('lol')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taking data from /work/FAC/FGSE/IDYST/tbeucler/default/fabien/repos/cleaner_version/data/datasets_1h\n",
      "With condition based on /work/FAC/FGSE/IDYST/tbeucler/default/fabien/repos/cleaner_version/pre_processing/tracks/ALL_TRACKS/tracks_1h_EU\n",
      "Finished filtering 10m_u_component_of_wind\n",
      "Finished filtering 10m_v_component_of_wind\n",
      "Finished filtering 2m_dewpoint_temperature\n",
      "Finished filtering 2m_temperature\n",
      "Finished filtering convective_available_potential_energy\n",
      "Finished filtering convective_precipitation\n",
      "Finished filtering convective_rain_rate\n",
      "Finished filtering convective_snowfall\n",
      "Finished filtering geopotential\n",
      "Finished filtering high_cloud_cover\n",
      "Finished filtering instantaneous_10m_wind_gust\n",
      "Finished filtering k_index\n",
      "Finished filtering large_scale_precipitation\n",
      "Finished filtering large_scale_snowfall\n",
      "Finished filtering mean_large_scale_precipitation_rate\n",
      "Finished filtering mean_top_net_long_wave_radiation_flux\n",
      "Finished filtering mean_top_net_short_wave_radiation_flux\n",
      "Finished filtering mean_total_precipitation_rate\n",
      "Finished filtering mean_sea_level_pressure\n",
      "Finished filtering mean_surface_latent_heat_flux\n",
      "Finished filtering mean_surface_sensible_heat_flux\n",
      "Finished filtering mean_surface_net_long_wave_radiation_flux\n",
      "Finished filtering mean_surface_net_short_wave_radiation_flux\n",
      "Finished filtering mean_vertically_integrated_moisture_divergence\n",
      "Finished filtering relative_humidity\n",
      "Finished filtering surface_pressure\n",
      "Finished filtering surface_latent_heat_flux\n",
      "Finished filtering surface_sensible_heat_flux\n",
      "Finished filtering specific_rain_water_content\n",
      "Finished filtering total_precipitation\n",
      "Finished filtering total_totals_index\n",
      "Finished filtering vertical_velocity\n"
     ]
    }
   ],
   "source": [
    "# test of the function\n",
    "filtering_storms(\n",
    "    variables,\n",
    "    '1h',\n",
    "    path,\n",
    "    'test/',\n",
    "    levels,\n",
    "    filter_type='non_EU',\n",
    "    continuous_EU=False,\n",
    "    continuous_non_EU=True,\n",
    "    all_details=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taking data from /work/FAC/FGSE/IDYST/tbeucler/default/fabien/repos/cleaner_version/data/datasets_1h\n",
      "With condition based on /work/FAC/FGSE/IDYST/tbeucler/default/fabien/repos/cleaner_version/pre_processing/tracks/ALL_TRACKS/tracks_1h_EU\n",
      "Finished filtering 10m_u_component_of_wind\n",
      "Finished filtering 10m_v_component_of_wind\n",
      "Finished filtering 2m_dewpoint_temperature\n",
      "Finished filtering 2m_temperature\n",
      "Finished filtering convective_available_potential_energy\n",
      "Finished filtering convective_precipitation\n",
      "Finished filtering convective_rain_rate\n",
      "Finished filtering convective_snowfall\n",
      "Finished filtering geopotential\n",
      "Finished filtering high_cloud_cover\n",
      "Finished filtering instantaneous_10m_wind_gust\n",
      "Finished filtering k_index\n",
      "Finished filtering large_scale_precipitation\n",
      "Finished filtering large_scale_snowfall\n",
      "Finished filtering mean_large_scale_precipitation_rate\n",
      "Finished filtering mean_top_net_long_wave_radiation_flux\n",
      "Finished filtering mean_top_net_short_wave_radiation_flux\n",
      "Finished filtering mean_total_precipitation_rate\n",
      "Finished filtering mean_sea_level_pressure\n",
      "Finished filtering mean_surface_latent_heat_flux\n",
      "Finished filtering mean_surface_sensible_heat_flux\n",
      "Finished filtering mean_surface_net_long_wave_radiation_flux\n",
      "Finished filtering mean_surface_net_short_wave_radiation_flux\n",
      "Finished filtering mean_vertically_integrated_moisture_divergence\n",
      "Finished filtering relative_humidity\n",
      "Finished filtering surface_pressure\n",
      "Finished filtering surface_latent_heat_flux\n",
      "Finished filtering surface_sensible_heat_flux\n",
      "Finished filtering specific_rain_water_content\n",
      "Finished filtering total_precipitation\n",
      "Finished filtering total_totals_index\n",
      "Finished filtering vertical_velocity\n"
     ]
    }
   ],
   "source": [
    "# creation of the EU dataset with timestep 1 hour\n",
    "\n",
    "extraction_squares.filtering_EU_storms(f'{path}data/variable_list_SL.csv', '1h',path,'data/', levels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taking data from /work/FAC/FGSE/IDYST/tbeucler/default/fabien/repos/cleaner_version/data/datasets_1h\n",
      "With condition based on /work/FAC/FGSE/IDYST/tbeucler/default/fabien/repos/cleaner_version/pre_processing/tracks/ALL_TRACKS/tracks_1h_EU\n",
      "Finished filtering 10m_u_component_of_wind\n",
      "Finished filtering 10m_v_component_of_wind\n",
      "Finished filtering 2m_dewpoint_temperature\n",
      "Finished filtering 2m_temperature\n",
      "Finished filtering convective_available_potential_energy\n",
      "Finished filtering convective_precipitation\n",
      "Finished filtering convective_rain_rate\n",
      "Finished filtering convective_snowfall\n",
      "Finished filtering geopotential\n",
      "Finished filtering high_cloud_cover\n",
      "Finished filtering instantaneous_10m_wind_gust\n",
      "Finished filtering k_index\n",
      "Finished filtering large_scale_precipitation\n",
      "Finished filtering large_scale_snowfall\n",
      "Finished filtering mean_large_scale_precipitation_rate\n",
      "Finished filtering mean_top_net_long_wave_radiation_flux\n",
      "Finished filtering mean_top_net_short_wave_radiation_flux\n",
      "Finished filtering mean_total_precipitation_rate\n",
      "Finished filtering mean_sea_level_pressure\n",
      "Finished filtering mean_surface_latent_heat_flux\n",
      "Finished filtering mean_surface_sensible_heat_flux\n",
      "Finished filtering mean_surface_net_long_wave_radiation_flux\n",
      "Finished filtering mean_surface_net_short_wave_radiation_flux\n",
      "Finished filtering mean_vertically_integrated_moisture_divergence\n",
      "Finished filtering relative_humidity\n",
      "Finished filtering surface_pressure\n",
      "Finished filtering surface_latent_heat_flux\n",
      "Finished filtering surface_sensible_heat_flux\n",
      "Finished filtering specific_rain_water_content\n",
      "Finished filtering total_precipitation\n",
      "Finished filtering total_totals_index\n",
      "Finished filtering vertical_velocity\n"
     ]
    }
   ],
   "source": [
    "# creation of the non EU dataset with timestep 1 hour\n",
    "\n",
    "extraction_squares.filtering_non_EU_storms(f'{path}data/variable_list_SL.csv', '1h',path,'data/', levels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
