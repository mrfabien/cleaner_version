{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "from datetime import datetime\n",
    "#from guppy import hpy\n",
    "import os\n",
    "\n",
    "dataset_path = '/work/FAC/FGSE/IDYST/tbeucler/default/fabien/repos/cleaner_version/data/datasets_1h'\n",
    "track_path = '/work/FAC/FGSE/IDYST/tbeucler/default/fabien/repos/cleaner_version/pre_processing/tracks/ALL_TRACKS/tracks_1h'\n",
    "\n",
    "# Créer une instance de heapy\n",
    "#hp = hpy()\n",
    "# DOESN'T WORK 10m_v_component_of_wind, 2m_temperature, 10m_u_component_of_wind, 2m_dewpoint_temperature for 2020: too large\n",
    "# Define a function to open datasets and concatenate them\n",
    "def open_and_concatenate(year, variable, months, way, level=0):\n",
    "    datasets = [xr.open_dataset(f'{way}{variable}/ERA5_{year}-{month}_{variable}.nc') for month in months]\n",
    "    if variable == 'geopotential' and level != 0:\n",
    "        datasets = [dataset.sel(level=level) for dataset in datasets]\n",
    "    return xr.concat(datasets, dim='time')\n",
    "\n",
    "# Define a function to calculate statistics\n",
    "def calculate_statistics(data_array):\n",
    "    return {\n",
    "        'mean': np.mean(data_array),\n",
    "        'min': np.min(data_array),\n",
    "        'max': np.max(data_array),\n",
    "        'std': np.std(data_array),\n",
    "    }\n",
    "\n",
    "# Function to log processing details\n",
    "def log_processing(variable, year, level, storm_number):\n",
    "    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    log_message = f'Processed variable: {variable}, Year: {year}, Level: {level}, Timestamp: {timestamp}, Storm number:{storm_number}'\n",
    "    with open(f'{dataset_path}/processing_log.txt', 'a') as log_file:\n",
    "        log_file.write(log_message + '\\n')\n",
    "\n",
    "# Créer une instance de heapy\n",
    "#hp = hpy()\n",
    "\n",
    "# Function to check if all CSV files exist\n",
    "def all_csv_files_exist(variable, year, level):\n",
    "    directory = f'{dataset_path}/{variable}'\n",
    "    if not os.path.exists(directory):\n",
    "        return False\n",
    "\n",
    "    for storm_dir in os.listdir(directory):\n",
    "        storm_path = os.path.join(directory, storm_dir)\n",
    "        if os.path.isdir(storm_path):\n",
    "            for stat in ['mean', 'min', 'max', 'std']:\n",
    "                file_path = os.path.join(storm_path, f'{stat}_{storm_dir.split(\"_\")[1]}_{level}.csv')\n",
    "                if not os.path.exists(file_path):\n",
    "                    return False\n",
    "    return True\n",
    "\n",
    "variable = 'mean_surface_sensible_heat_flux'#sys.argv[1]\n",
    "year = 1991#sys.argv[2]\n",
    "level = 0#int(sys.argv[3])\n",
    "\n",
    "# Main function to process data\n",
    "#def process_data(variable, year, level=0):\n",
    "year = int(year)\n",
    "year_next = year + 1\n",
    "month_act = [10, 11, 12]\n",
    "month_next = [1, 2, 3]\n",
    "if variable == 'geopotential':\n",
    "    way = '/work/FAC/FGSE/IDYST/tbeucler/default/raw_data/ECMWF/ERA5_hourly_PL/'\n",
    "else:\n",
    "    way = '/work/FAC/FGSE/IDYST/tbeucler/default/raw_data/ECMWF/ERA5/SL/'\n",
    "\n",
    "# Open and concatenate datasets\n",
    "if year == 1990:\n",
    "    dataset_act = open_and_concatenate(str(year), variable, month_next, way, level)\n",
    "    dataset_next = open_and_concatenate(str(year_next), variable, month_next, way, level)\n",
    "    try:\n",
    "        dataset = xr.concat([dataset_act, dataset_next], dim='time')\n",
    "    except:\n",
    "        dataset = xr.concat([dataset_act, dataset_next], dim='valid_time')\n",
    "    dataset = dataset.chunk({'time': 10})\n",
    "    print('year == 1990')\n",
    "elif year == 2021:\n",
    "    dataset = open_and_concatenate(str(year), variable, month_next, way, level)\n",
    "else:\n",
    "    dataset_act = open_and_concatenate(str(year), variable, month_act, way, level)\n",
    "    dataset_next = open_and_concatenate(str(year_next), variable, month_next, way, level)\n",
    "    try:\n",
    "        dataset = xr.concat([dataset_act, dataset_next], dim='time')\n",
    "    except:\n",
    "        dataset = xr.concat([dataset_act, dataset_next], dim='valid_time')\n",
    "    dataset = dataset.chunk({'time': 10})\n",
    "    print('year != 1990')\n",
    "\n",
    "# Determine the specific variable to extract\n",
    "try: \n",
    "    specific_var = next(var for var in dataset.variables if var not in ['longitude', 'latitude', 'time', 'level'])\n",
    "except:\n",
    "    specific_var = next(var for var in dataset.variables if var not in ['longitude', 'latitude', 'valid_time', 'level'])\n",
    "    print(specific_var)\n",
    "\n",
    "# Import all tracks and convert dates\n",
    "dates = pd.read_csv(f'/work/FAC/FGSE/IDYST/tbeucler/default/fabien/repos/cleaner_version/pre_processing/tracks/storm_dates.csv', \n",
    "                parse_dates=['start_date', 'end_date'])\n",
    "dates['year'] = dates['start_date'].dt.year\n",
    "try:\n",
    "    dates.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Find the indices for storms within the specified timeframe\n",
    "if year == 1990:\n",
    "    index_start_october = dates[(dates['start_date'].dt.month <= 3) & (dates['start_date'].dt.year == year)].index[0]\n",
    "    index_end_march = dates[(dates['end_date'].dt.month <= 3) & (dates['end_date'].dt.year == year_next)].index[0]\n",
    "elif year == 2021:\n",
    "    index_start_october = dates[(dates['start_date'].dt.month <= 3) & (dates['start_date'].dt.year == year)].index[0]\n",
    "    index_end_march = dates[(dates['end_date'].dt.year == 2021)].index[0]\n",
    "else:\n",
    "# Chercher start_october dans year, sinon chercher dès janvier de year_next\n",
    "    index_start_october = dates[((dates['start_date'].dt.month >= 10) & (dates['start_date'].dt.year == year)) | ((dates['start_date'].dt.year == year_next) & (dates['start_date'].dt.month >= 1))].index[0]\n",
    "    index_end_march_first = dates[((dates['end_date'].dt.month <= 3) & (dates['end_date'].dt.year == year_next))].index\n",
    "    #print(index_start_october, index_end_march_first, '3rd condition start_october + index_end_march_first')\n",
    "    if len(index_end_march_first) > 0:\n",
    "        index_end_march = index_end_march_first[-1]\n",
    "        #print(index_end_march, 'index_end_march 1st condition of 2nd condition')\n",
    "    else:\n",
    "        # Si year_next ne renvoie rien, chercher la dernière instance de tempête dans year\n",
    "        index_end_march = dates[((dates['end_date'].dt.year == year) & (dates['end_date'].dt.month <= 12))].index[-1]\n",
    "        #print(index_end_march, 'index_end_march 2nd condition of 2nd condition')\n",
    "# Process each storm\n",
    "for i in range(index_start_october, index_end_march + 1):\n",
    "    storm_number = dates.at[i, 'storm_index']\n",
    "    track = pd.read_csv(f'{track_path}/storm_{storm_number}.csv')\n",
    "    start_date = dates.at[i, 'start_date']\n",
    "    end_date = dates.at[i, 'end_date']\n",
    "    storm_data = dataset[specific_var].sel(time=slice(start_date, end_date))\n",
    "    print(i, 'indice de la tempête')\n",
    "\n",
    "    # Initialize lists to store statistics\n",
    "    stats = {'mean': [], 'min': [], 'max': [], 'std': []}\n",
    "    #, 'skewness': [], 'kurtosis': []\n",
    "\n",
    "    # Process each time step\n",
    "    for t_index in range(0, len(storm_data.time)):#, time_step in enumerate(storm_data.time):\n",
    "        #data_slice = storm_data.sel(time=time_step).values\n",
    "\n",
    "        # Extract coordinates for the current time step\n",
    "\n",
    "        lon_e_temp = track.loc[t_index, 'lon_east']\n",
    "        lon_w_temp = track.loc[t_index, 'lon_west']\n",
    "        lat_s_temp = track.loc[t_index, 'lat_south']\n",
    "        lat_n_temp = track.loc[t_index, 'lat_north']\n",
    "\n",
    "        lon_test = np.asanyarray(storm_data.longitude[:])\n",
    "        lat_test = np.asanyarray(storm_data.latitude[:])\n",
    "\n",
    "        closest_lon_w = np.abs(lon_test - lon_w_temp).argmin()\n",
    "        closest_lon_e = np.abs(lon_test - lon_e_temp).argmin()\n",
    "        closest_lat_s = np.abs(lat_test - lat_s_temp).argmin()\n",
    "        closest_lat_n = np.abs(lat_test - lat_n_temp).argmin()\n",
    "\n",
    "        closest_lon_w_coor = lon_test[closest_lon_w]\n",
    "        closest_lon_e_coor = lon_test[closest_lon_e]\n",
    "        closest_lat_s_coor = lat_test[closest_lat_s]\n",
    "        closest_lat_n_coor = lat_test[closest_lat_n]\n",
    "\n",
    "        # Use .roll to handle the 0°/360° boundary\n",
    "        if closest_lon_w_coor < 100 and closest_lon_e_coor > 100:\n",
    "            roll_shift = {'longitude': int(round(closest_lon_w_coor, 0)), 'longitude': int(round(closest_lon_e_coor, 0))}\n",
    "            storm_data_rolled = storm_data.roll(roll_shift, roll_coords=True)\n",
    "        else:\n",
    "            storm_data_rolled = storm_data\n",
    "\n",
    "        # Slice the dataset based on the rolled longitudes and latitudes\n",
    "        temp_ds_time = storm_data_rolled.isel(time=t_index)#[specific_var].isel(time=t_index)\n",
    "        temp_ds = temp_ds_time.sel(latitude=slice(closest_lat_n_coor, closest_lat_s_coor),\n",
    "                                    longitude=slice(closest_lon_e_coor, closest_lon_w_coor)).values\n",
    "\n",
    "        # Calculate statistics for the sliced data\n",
    "        step_stats = calculate_statistics(temp_ds)\n",
    "        for key in stats:\n",
    "            stats[key].append(step_stats[key])\n",
    "\n",
    "    # Save statistics to CSV files\n",
    "    for key in stats:\n",
    "        directory = f'{dataset_path}/{variable}/storm_{storm_number}'\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "        pd.DataFrame(stats[key]).to_csv(f'{directory}/{key}_{storm_number}_{level}.csv')\n",
    "        print('done', key)\n",
    "\n",
    "    # Log the processing details\n",
    "    #log_processing(variable, year, level, storm_number)\n",
    "\n",
    "#if __name__ == '__main__':\n",
    "\n",
    "    #process_data(variable, year, level)\n",
    "\n",
    "'''if __name__ == '__main__':\n",
    "    variable = sys.argv[1]\n",
    "    year = sys.argv[2]\n",
    "    level = int(sys.argv[3])\n",
    "\n",
    "    if not all_csv_files_exist(variable, year, level):\n",
    "        process_data(variable, year, level)\n",
    "    else:\n",
    "        print(f'All CSV files for variable: {variable}, year: {year}, and level: {level} already exist. Skipping processing.')'''\n",
    "\n",
    "# Obtenir un instantané de l'utilisation de la mémoire\n",
    "#h = hp.heap()\n",
    "\n",
    "# Imprimer l'information d'utilisation de la mémoire\n",
    "#print(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "from datetime import datetime\n",
    "#from guppy import hpy\n",
    "import os\n",
    "\n",
    "def open_monthly_nc(variable, year, months, way, level=0):\n",
    "    datasets = [xr.open_dataset(f'{way}{variable}/ERA5_{year}-{month}_{variable}.nc') for month in months]\n",
    "    if variable == 'geopotential' and level != 0:\n",
    "        datasets = [dataset.sel(level=level) for dataset in datasets]\n",
    "    keys = list(datasets[0].coords.keys())\n",
    "    if keys == ['number', 'valid_time', 'latitude', 'longitude', 'expver']:\n",
    "        dim = 'valid_time'\n",
    "    else:\n",
    "        dim = 'time'\n",
    "    concated_datasets = xr.concat(datasets, dim=dim)\n",
    "    return concated_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test = open_monthly_nc('mean_surface_sensible_heat_flux','1990', [1, 2, 3], '/work/FAC/FGSE/IDYST/tbeucler/default/raw_data/ECMWF/ERA5/SL/', 0)\n",
    "\n",
    "test_1 = open_monthly_nc('mean_surface_sensible_heat_flux','1990', [1, 2, 3], '/work/FAC/FGSE/IDYST/tbeucler/default/raw_data/ECMWF/ERA5/SL/')\n",
    "#test_2 = xr.open_dataset('/work/FAC/FGSE/IDYST/tbeucler/default/raw_data/ECMWF/ERA5/SL/mean_surface_sensible_heat_flux/ERA5_1990-2_mean_surface_sensible_heat_flux.nc')\n",
    "#test_3 = xr.open_dataset('/work/FAC/FGSE/IDYST/tbeucler/default/raw_data/ECMWF/ERA5/SL/mean_surface_sensible_heat_flux/ERA5_1990-3_mean_surface_sensible_heat_flux.nc')\n",
    "\n",
    "#concatenate = xr.concat([test_1, test_2, test_3], dim='valid_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n"
     ]
    }
   ],
   "source": [
    "keys = list(concatenate.coords.keys())\n",
    "keys\n",
    "if keys == ['number', 'valid_time', 'latitude', 'longitude', 'expver']:\n",
    "    dim = 'valid_time'\n",
    "else:\n",
    "    dim = 'time'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><svg style=\"position: absolute; width: 0; height: 0; overflow: hidden\">\n",
       "<defs>\n",
       "<symbol id=\"icon-database\" viewBox=\"0 0 32 32\">\n",
       "<path d=\"M16 0c-8.837 0-16 2.239-16 5v4c0 2.761 7.163 5 16 5s16-2.239 16-5v-4c0-2.761-7.163-5-16-5z\"></path>\n",
       "<path d=\"M16 17c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
       "<path d=\"M16 26c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
       "</symbol>\n",
       "<symbol id=\"icon-file-text2\" viewBox=\"0 0 32 32\">\n",
       "<path d=\"M28.681 7.159c-0.694-0.947-1.662-2.053-2.724-3.116s-2.169-2.030-3.116-2.724c-1.612-1.182-2.393-1.319-2.841-1.319h-15.5c-1.378 0-2.5 1.121-2.5 2.5v27c0 1.378 1.122 2.5 2.5 2.5h23c1.378 0 2.5-1.122 2.5-2.5v-19.5c0-0.448-0.137-1.23-1.319-2.841zM24.543 5.457c0.959 0.959 1.712 1.825 2.268 2.543h-4.811v-4.811c0.718 0.556 1.584 1.309 2.543 2.268zM28 29.5c0 0.271-0.229 0.5-0.5 0.5h-23c-0.271 0-0.5-0.229-0.5-0.5v-27c0-0.271 0.229-0.5 0.5-0.5 0 0 15.499-0 15.5 0v7c0 0.552 0.448 1 1 1h7v19.5z\"></path>\n",
       "<path d=\"M23 26h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "<path d=\"M23 22h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "<path d=\"M23 18h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "</symbol>\n",
       "</defs>\n",
       "</svg>\n",
       "<style>/* CSS stylesheet for displaying xarray objects in jupyterlab.\n",
       " *\n",
       " */\n",
       "\n",
       ":root {\n",
       "  --xr-font-color0: var(--jp-content-font-color0, rgba(0, 0, 0, 1));\n",
       "  --xr-font-color2: var(--jp-content-font-color2, rgba(0, 0, 0, 0.54));\n",
       "  --xr-font-color3: var(--jp-content-font-color3, rgba(0, 0, 0, 0.38));\n",
       "  --xr-border-color: var(--jp-border-color2, #e0e0e0);\n",
       "  --xr-disabled-color: var(--jp-layout-color3, #bdbdbd);\n",
       "  --xr-background-color: var(--jp-layout-color0, white);\n",
       "  --xr-background-color-row-even: var(--jp-layout-color1, white);\n",
       "  --xr-background-color-row-odd: var(--jp-layout-color2, #eeeeee);\n",
       "}\n",
       "\n",
       "html[theme=dark],\n",
       "html[data-theme=dark],\n",
       "body[data-theme=dark],\n",
       "body.vscode-dark {\n",
       "  --xr-font-color0: rgba(255, 255, 255, 1);\n",
       "  --xr-font-color2: rgba(255, 255, 255, 0.54);\n",
       "  --xr-font-color3: rgba(255, 255, 255, 0.38);\n",
       "  --xr-border-color: #1F1F1F;\n",
       "  --xr-disabled-color: #515151;\n",
       "  --xr-background-color: #111111;\n",
       "  --xr-background-color-row-even: #111111;\n",
       "  --xr-background-color-row-odd: #313131;\n",
       "}\n",
       "\n",
       ".xr-wrap {\n",
       "  display: block !important;\n",
       "  min-width: 300px;\n",
       "  max-width: 700px;\n",
       "}\n",
       "\n",
       ".xr-text-repr-fallback {\n",
       "  /* fallback to plain text repr when CSS is not injected (untrusted notebook) */\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-header {\n",
       "  padding-top: 6px;\n",
       "  padding-bottom: 6px;\n",
       "  margin-bottom: 4px;\n",
       "  border-bottom: solid 1px var(--xr-border-color);\n",
       "}\n",
       "\n",
       ".xr-header > div,\n",
       ".xr-header > ul {\n",
       "  display: inline;\n",
       "  margin-top: 0;\n",
       "  margin-bottom: 0;\n",
       "}\n",
       "\n",
       ".xr-obj-type,\n",
       ".xr-array-name {\n",
       "  margin-left: 2px;\n",
       "  margin-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-obj-type {\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-sections {\n",
       "  padding-left: 0 !important;\n",
       "  display: grid;\n",
       "  grid-template-columns: 150px auto auto 1fr 0 20px 0 20px;\n",
       "}\n",
       "\n",
       ".xr-section-item {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-section-item input {\n",
       "  display: inline-block;\n",
       "  opacity: 0;\n",
       "}\n",
       "\n",
       ".xr-section-item input + label {\n",
       "  color: var(--xr-disabled-color);\n",
       "}\n",
       "\n",
       ".xr-section-item input:enabled + label {\n",
       "  cursor: pointer;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-section-item input:focus + label {\n",
       "  border: 2px solid var(--xr-font-color0);\n",
       "}\n",
       "\n",
       ".xr-section-item input:enabled + label:hover {\n",
       "  color: var(--xr-font-color0);\n",
       "}\n",
       "\n",
       ".xr-section-summary {\n",
       "  grid-column: 1;\n",
       "  color: var(--xr-font-color2);\n",
       "  font-weight: 500;\n",
       "}\n",
       "\n",
       ".xr-section-summary > span {\n",
       "  display: inline-block;\n",
       "  padding-left: 0.5em;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:disabled + label {\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-section-summary-in + label:before {\n",
       "  display: inline-block;\n",
       "  content: '►';\n",
       "  font-size: 11px;\n",
       "  width: 15px;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:disabled + label:before {\n",
       "  color: var(--xr-disabled-color);\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked + label:before {\n",
       "  content: '▼';\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked + label > span {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-section-summary,\n",
       ".xr-section-inline-details {\n",
       "  padding-top: 4px;\n",
       "  padding-bottom: 4px;\n",
       "}\n",
       "\n",
       ".xr-section-inline-details {\n",
       "  grid-column: 2 / -1;\n",
       "}\n",
       "\n",
       ".xr-section-details {\n",
       "  display: none;\n",
       "  grid-column: 1 / -1;\n",
       "  margin-bottom: 5px;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked ~ .xr-section-details {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-array-wrap {\n",
       "  grid-column: 1 / -1;\n",
       "  display: grid;\n",
       "  grid-template-columns: 20px auto;\n",
       "}\n",
       "\n",
       ".xr-array-wrap > label {\n",
       "  grid-column: 1;\n",
       "  vertical-align: top;\n",
       "}\n",
       "\n",
       ".xr-preview {\n",
       "  color: var(--xr-font-color3);\n",
       "}\n",
       "\n",
       ".xr-array-preview,\n",
       ".xr-array-data {\n",
       "  padding: 0 5px !important;\n",
       "  grid-column: 2;\n",
       "}\n",
       "\n",
       ".xr-array-data,\n",
       ".xr-array-in:checked ~ .xr-array-preview {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-array-in:checked ~ .xr-array-data,\n",
       ".xr-array-preview {\n",
       "  display: inline-block;\n",
       "}\n",
       "\n",
       ".xr-dim-list {\n",
       "  display: inline-block !important;\n",
       "  list-style: none;\n",
       "  padding: 0 !important;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".xr-dim-list li {\n",
       "  display: inline-block;\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".xr-dim-list:before {\n",
       "  content: '(';\n",
       "}\n",
       "\n",
       ".xr-dim-list:after {\n",
       "  content: ')';\n",
       "}\n",
       "\n",
       ".xr-dim-list li:not(:last-child):after {\n",
       "  content: ',';\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".xr-has-index {\n",
       "  font-weight: bold;\n",
       "}\n",
       "\n",
       ".xr-var-list,\n",
       ".xr-var-item {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-var-item > div,\n",
       ".xr-var-item label,\n",
       ".xr-var-item > .xr-var-name span {\n",
       "  background-color: var(--xr-background-color-row-even);\n",
       "  margin-bottom: 0;\n",
       "}\n",
       "\n",
       ".xr-var-item > .xr-var-name:hover span {\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".xr-var-list > li:nth-child(odd) > div,\n",
       ".xr-var-list > li:nth-child(odd) > label,\n",
       ".xr-var-list > li:nth-child(odd) > .xr-var-name span {\n",
       "  background-color: var(--xr-background-color-row-odd);\n",
       "}\n",
       "\n",
       ".xr-var-name {\n",
       "  grid-column: 1;\n",
       "}\n",
       "\n",
       ".xr-var-dims {\n",
       "  grid-column: 2;\n",
       "}\n",
       "\n",
       ".xr-var-dtype {\n",
       "  grid-column: 3;\n",
       "  text-align: right;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-var-preview {\n",
       "  grid-column: 4;\n",
       "}\n",
       "\n",
       ".xr-index-preview {\n",
       "  grid-column: 2 / 5;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-var-name,\n",
       ".xr-var-dims,\n",
       ".xr-var-dtype,\n",
       ".xr-preview,\n",
       ".xr-attrs dt {\n",
       "  white-space: nowrap;\n",
       "  overflow: hidden;\n",
       "  text-overflow: ellipsis;\n",
       "  padding-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-var-name:hover,\n",
       ".xr-var-dims:hover,\n",
       ".xr-var-dtype:hover,\n",
       ".xr-attrs dt:hover {\n",
       "  overflow: visible;\n",
       "  width: auto;\n",
       "  z-index: 1;\n",
       "}\n",
       "\n",
       ".xr-var-attrs,\n",
       ".xr-var-data,\n",
       ".xr-index-data {\n",
       "  display: none;\n",
       "  background-color: var(--xr-background-color) !important;\n",
       "  padding-bottom: 5px !important;\n",
       "}\n",
       "\n",
       ".xr-var-attrs-in:checked ~ .xr-var-attrs,\n",
       ".xr-var-data-in:checked ~ .xr-var-data,\n",
       ".xr-index-data-in:checked ~ .xr-index-data {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       ".xr-var-data > table {\n",
       "  float: right;\n",
       "}\n",
       "\n",
       ".xr-var-name span,\n",
       ".xr-var-data,\n",
       ".xr-index-name div,\n",
       ".xr-index-data,\n",
       ".xr-attrs {\n",
       "  padding-left: 25px !important;\n",
       "}\n",
       "\n",
       ".xr-attrs,\n",
       ".xr-var-attrs,\n",
       ".xr-var-data,\n",
       ".xr-index-data {\n",
       "  grid-column: 1 / -1;\n",
       "}\n",
       "\n",
       "dl.xr-attrs {\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "  display: grid;\n",
       "  grid-template-columns: 125px auto;\n",
       "}\n",
       "\n",
       ".xr-attrs dt,\n",
       ".xr-attrs dd {\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "  float: left;\n",
       "  padding-right: 10px;\n",
       "  width: auto;\n",
       "}\n",
       "\n",
       ".xr-attrs dt {\n",
       "  font-weight: normal;\n",
       "  grid-column: 1;\n",
       "}\n",
       "\n",
       ".xr-attrs dt:hover span {\n",
       "  display: inline-block;\n",
       "  background: var(--xr-background-color);\n",
       "  padding-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-attrs dd {\n",
       "  grid-column: 2;\n",
       "  white-space: pre-wrap;\n",
       "  word-break: break-all;\n",
       "}\n",
       "\n",
       ".xr-icon-database,\n",
       ".xr-icon-file-text2,\n",
       ".xr-no-icon {\n",
       "  display: inline-block;\n",
       "  vertical-align: middle;\n",
       "  width: 1em;\n",
       "  height: 1.5em !important;\n",
       "  stroke-width: 0;\n",
       "  stroke: currentColor;\n",
       "  fill: currentColor;\n",
       "}\n",
       "</style><pre class='xr-text-repr-fallback'>&lt;xarray.Dataset&gt; Size: 9GB\n",
       "Dimensions:     (valid_time: 2160, latitude: 721, longitude: 1440)\n",
       "Coordinates:\n",
       "    number      int64 8B 0\n",
       "  * valid_time  (valid_time) datetime64[ns] 17kB 1990-01-01 ... 1990-03-31T23...\n",
       "  * latitude    (latitude) float64 6kB 90.0 89.75 89.5 ... -89.5 -89.75 -90.0\n",
       "  * longitude   (longitude) float64 12kB 0.0 0.25 0.5 0.75 ... 359.2 359.5 359.8\n",
       "    expver      (valid_time) &lt;U4 35kB &#x27;0001&#x27; &#x27;0001&#x27; &#x27;0001&#x27; ... &#x27;0001&#x27; &#x27;0001&#x27;\n",
       "Data variables:\n",
       "    msshf       (valid_time, latitude, longitude) float32 9GB 4.759 ... 22.54\n",
       "Attributes:\n",
       "    GRIB_centre:             ecmf\n",
       "    GRIB_centreDescription:  European Centre for Medium-Range Weather Forecasts\n",
       "    GRIB_subCentre:          0\n",
       "    Conventions:             CF-1.7\n",
       "    institution:             European Centre for Medium-Range Weather Forecasts\n",
       "    history:                 2024-11-22T11:17 GRIB to CDM+CF via cfgrib-0.9.1...</pre><div class='xr-wrap' style='display:none'><div class='xr-header'><div class='xr-obj-type'>xarray.Dataset</div></div><ul class='xr-sections'><li class='xr-section-item'><input id='section-da5bfa50-b8dc-4b8b-9e48-09fb3b1f28c2' class='xr-section-summary-in' type='checkbox' disabled ><label for='section-da5bfa50-b8dc-4b8b-9e48-09fb3b1f28c2' class='xr-section-summary'  title='Expand/collapse section'>Dimensions:</label><div class='xr-section-inline-details'><ul class='xr-dim-list'><li><span class='xr-has-index'>valid_time</span>: 2160</li><li><span class='xr-has-index'>latitude</span>: 721</li><li><span class='xr-has-index'>longitude</span>: 1440</li></ul></div><div class='xr-section-details'></div></li><li class='xr-section-item'><input id='section-636b63d3-5711-46bb-b11c-0ebe1a115606' class='xr-section-summary-in' type='checkbox'  checked><label for='section-636b63d3-5711-46bb-b11c-0ebe1a115606' class='xr-section-summary' >Coordinates: <span>(5)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-var-name'><span>number</span></div><div class='xr-var-dims'>()</div><div class='xr-var-dtype'>int64</div><div class='xr-var-preview xr-preview'>0</div><input id='attrs-2cc1826e-60ed-4b77-958e-225831db72d4' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-2cc1826e-60ed-4b77-958e-225831db72d4' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-15b187c6-093d-4105-af27-e4f829ad586c' class='xr-var-data-in' type='checkbox'><label for='data-15b187c6-093d-4105-af27-e4f829ad586c' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>long_name :</span></dt><dd>ensemble member numerical id</dd><dt><span>units :</span></dt><dd>1</dd><dt><span>standard_name :</span></dt><dd>realization</dd></dl></div><div class='xr-var-data'><pre>array(0)</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>valid_time</span></div><div class='xr-var-dims'>(valid_time)</div><div class='xr-var-dtype'>datetime64[ns]</div><div class='xr-var-preview xr-preview'>1990-01-01 ... 1990-03-31T23:00:00</div><input id='attrs-ef99e400-e551-4af4-ba52-fd0341bcf071' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-ef99e400-e551-4af4-ba52-fd0341bcf071' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-b55d6f95-3187-4d28-8c56-d2b2e6544e53' class='xr-var-data-in' type='checkbox'><label for='data-b55d6f95-3187-4d28-8c56-d2b2e6544e53' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>long_name :</span></dt><dd>time</dd><dt><span>standard_name :</span></dt><dd>time</dd></dl></div><div class='xr-var-data'><pre>array([&#x27;1990-01-01T00:00:00.000000000&#x27;, &#x27;1990-01-01T01:00:00.000000000&#x27;,\n",
       "       &#x27;1990-01-01T02:00:00.000000000&#x27;, ..., &#x27;1990-03-31T21:00:00.000000000&#x27;,\n",
       "       &#x27;1990-03-31T22:00:00.000000000&#x27;, &#x27;1990-03-31T23:00:00.000000000&#x27;],\n",
       "      dtype=&#x27;datetime64[ns]&#x27;)</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>latitude</span></div><div class='xr-var-dims'>(latitude)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>90.0 89.75 89.5 ... -89.75 -90.0</div><input id='attrs-f8d8102c-110b-424e-a7b0-7bf2fb24a0ec' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-f8d8102c-110b-424e-a7b0-7bf2fb24a0ec' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-73cafd85-44ec-4abf-a672-87039eaa85ab' class='xr-var-data-in' type='checkbox'><label for='data-73cafd85-44ec-4abf-a672-87039eaa85ab' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>units :</span></dt><dd>degrees_north</dd><dt><span>standard_name :</span></dt><dd>latitude</dd><dt><span>long_name :</span></dt><dd>latitude</dd><dt><span>stored_direction :</span></dt><dd>decreasing</dd></dl></div><div class='xr-var-data'><pre>array([ 90.  ,  89.75,  89.5 , ..., -89.5 , -89.75, -90.  ])</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>longitude</span></div><div class='xr-var-dims'>(longitude)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>0.0 0.25 0.5 ... 359.2 359.5 359.8</div><input id='attrs-3a1c1c6c-f24b-49c1-9e48-eafc620c3ea2' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-3a1c1c6c-f24b-49c1-9e48-eafc620c3ea2' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-59c0c061-5390-469a-9580-5f8850b400a7' class='xr-var-data-in' type='checkbox'><label for='data-59c0c061-5390-469a-9580-5f8850b400a7' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>units :</span></dt><dd>degrees_east</dd><dt><span>standard_name :</span></dt><dd>longitude</dd><dt><span>long_name :</span></dt><dd>longitude</dd></dl></div><div class='xr-var-data'><pre>array([0.0000e+00, 2.5000e-01, 5.0000e-01, ..., 3.5925e+02, 3.5950e+02,\n",
       "       3.5975e+02])</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>expver</span></div><div class='xr-var-dims'>(valid_time)</div><div class='xr-var-dtype'>&lt;U4</div><div class='xr-var-preview xr-preview'>&#x27;0001&#x27; &#x27;0001&#x27; ... &#x27;0001&#x27; &#x27;0001&#x27;</div><input id='attrs-671ab803-3d18-42b2-a410-2c362ba78ff8' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-671ab803-3d18-42b2-a410-2c362ba78ff8' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-5ad96529-f5ed-444c-a7dd-03129d411607' class='xr-var-data-in' type='checkbox'><label for='data-5ad96529-f5ed-444c-a7dd-03129d411607' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array([&#x27;0001&#x27;, &#x27;0001&#x27;, &#x27;0001&#x27;, ..., &#x27;0001&#x27;, &#x27;0001&#x27;, &#x27;0001&#x27;], dtype=&#x27;&lt;U4&#x27;)</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-7610dacf-5edd-410c-bc1f-b0120bc961c4' class='xr-section-summary-in' type='checkbox'  checked><label for='section-7610dacf-5edd-410c-bc1f-b0120bc961c4' class='xr-section-summary' >Data variables: <span>(1)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-var-name'><span>msshf</span></div><div class='xr-var-dims'>(valid_time, latitude, longitude)</div><div class='xr-var-dtype'>float32</div><div class='xr-var-preview xr-preview'>4.759 4.759 4.759 ... 22.54 22.54</div><input id='attrs-043d8cf1-c614-42d2-961d-8dbf18b79d04' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-043d8cf1-c614-42d2-961d-8dbf18b79d04' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-7ed9b137-f80b-48d8-a927-65364215c348' class='xr-var-data-in' type='checkbox'><label for='data-7ed9b137-f80b-48d8-a927-65364215c348' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>GRIB_paramId :</span></dt><dd>235033</dd><dt><span>GRIB_dataType :</span></dt><dd>fc</dd><dt><span>GRIB_numberOfPoints :</span></dt><dd>1038240</dd><dt><span>GRIB_typeOfLevel :</span></dt><dd>surface</dd><dt><span>GRIB_stepUnits :</span></dt><dd>1</dd><dt><span>GRIB_stepType :</span></dt><dd>avg</dd><dt><span>GRIB_gridType :</span></dt><dd>regular_ll</dd><dt><span>GRIB_uvRelativeToGrid :</span></dt><dd>0</dd><dt><span>GRIB_NV :</span></dt><dd>0</dd><dt><span>GRIB_Nx :</span></dt><dd>1440</dd><dt><span>GRIB_Ny :</span></dt><dd>721</dd><dt><span>GRIB_cfName :</span></dt><dd>unknown</dd><dt><span>GRIB_cfVarName :</span></dt><dd>msshf</dd><dt><span>GRIB_gridDefinitionDescription :</span></dt><dd>Latitude/Longitude Grid</dd><dt><span>GRIB_iDirectionIncrementInDegrees :</span></dt><dd>0.25</dd><dt><span>GRIB_iScansNegatively :</span></dt><dd>0</dd><dt><span>GRIB_jDirectionIncrementInDegrees :</span></dt><dd>0.25</dd><dt><span>GRIB_jPointsAreConsecutive :</span></dt><dd>0</dd><dt><span>GRIB_jScansPositively :</span></dt><dd>0</dd><dt><span>GRIB_latitudeOfFirstGridPointInDegrees :</span></dt><dd>90.0</dd><dt><span>GRIB_latitudeOfLastGridPointInDegrees :</span></dt><dd>-90.0</dd><dt><span>GRIB_longitudeOfFirstGridPointInDegrees :</span></dt><dd>0.0</dd><dt><span>GRIB_longitudeOfLastGridPointInDegrees :</span></dt><dd>359.75</dd><dt><span>GRIB_missingValue :</span></dt><dd>3.4028234663852886e+38</dd><dt><span>GRIB_name :</span></dt><dd>Mean surface sensible heat flux</dd><dt><span>GRIB_shortName :</span></dt><dd>msshf</dd><dt><span>GRIB_totalNumber :</span></dt><dd>0</dd><dt><span>GRIB_units :</span></dt><dd>W m**-2</dd><dt><span>long_name :</span></dt><dd>Mean surface sensible heat flux</dd><dt><span>units :</span></dt><dd>W m**-2</dd><dt><span>standard_name :</span></dt><dd>unknown</dd><dt><span>GRIB_surface :</span></dt><dd>0.0</dd></dl></div><div class='xr-var-data'><pre>array([[[  4.7592773,   4.7592773,   4.7592773, ...,   4.7592773,\n",
       "           4.7592773,   4.7592773],\n",
       "        [  4.7280273,   4.7436523,   4.7592773, ...,   4.6186523,\n",
       "           4.6499023,   4.6967773],\n",
       "        [  1.6499023,   1.6967773,   1.7436523, ...,   1.4311523,\n",
       "           1.5092773,   1.5717773],\n",
       "        ...,\n",
       "        [ 12.806152 ,  12.806152 ,  12.806152 , ...,  12.790527 ,\n",
       "          12.790527 ,  12.790527 ],\n",
       "        [ 12.649902 ,  12.649902 ,  12.649902 , ...,  12.649902 ,\n",
       "          12.649902 ,  12.649902 ],\n",
       "        [ 13.634277 ,  13.634277 ,  13.634277 , ...,  13.634277 ,\n",
       "          13.634277 ,  13.634277 ]],\n",
       "\n",
       "       [[  4.6308594,   4.6308594,   4.6308594, ...,   4.6308594,\n",
       "           4.6308594,   4.6308594],\n",
       "        [  4.4902344,   4.5058594,   4.5214844, ...,   4.3808594,\n",
       "           4.4277344,   4.4589844],\n",
       "        [  1.7714844,   1.8183594,   1.8496094, ...,   1.5527344,\n",
       "           1.6308594,   1.6933594],\n",
       "...\n",
       "        [ 22.744873 ,  22.729248 ,  22.744873 , ...,  22.604248 ,\n",
       "          22.651123 ,  22.697998 ],\n",
       "        [ 24.869873 ,  24.869873 ,  24.869873 , ...,  24.791748 ,\n",
       "          24.822998 ,  24.838623 ],\n",
       "        [ 22.072998 ,  22.072998 ,  22.072998 , ...,  22.072998 ,\n",
       "          22.072998 ,  22.072998 ]],\n",
       "\n",
       "       [[-22.522705 , -22.522705 , -22.522705 , ..., -22.522705 ,\n",
       "         -22.522705 , -22.522705 ],\n",
       "        [-24.97583  , -24.960205 , -24.960205 , ..., -25.022705 ,\n",
       "         -25.00708  , -24.991455 ],\n",
       "        [-27.897705 , -27.928955 , -27.928955 , ..., -27.97583  ,\n",
       "         -27.94458  , -27.91333  ],\n",
       "        ...,\n",
       "        [ 23.55542  ,  23.55542  ,  23.55542  , ...,  23.46167  ,\n",
       "          23.49292  ,  23.52417  ],\n",
       "        [ 25.43042  ,  25.43042  ,  25.43042  , ...,  25.383545 ,\n",
       "          25.39917  ,  25.414795 ],\n",
       "        [ 22.539795 ,  22.539795 ,  22.539795 , ...,  22.539795 ,\n",
       "          22.539795 ,  22.539795 ]]], dtype=float32)</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-8d91bd91-1c6f-40e1-8b75-fec921ad8e90' class='xr-section-summary-in' type='checkbox'  ><label for='section-8d91bd91-1c6f-40e1-8b75-fec921ad8e90' class='xr-section-summary' >Indexes: <span>(3)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-index-name'><div>valid_time</div></div><div class='xr-index-preview'>PandasIndex</div><div></div><input id='index-686d5c1b-796b-4be5-be3b-8e804373e584' class='xr-index-data-in' type='checkbox'/><label for='index-686d5c1b-796b-4be5-be3b-8e804373e584' title='Show/Hide index repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-index-data'><pre>PandasIndex(DatetimeIndex([&#x27;1990-01-01 00:00:00&#x27;, &#x27;1990-01-01 01:00:00&#x27;,\n",
       "               &#x27;1990-01-01 02:00:00&#x27;, &#x27;1990-01-01 03:00:00&#x27;,\n",
       "               &#x27;1990-01-01 04:00:00&#x27;, &#x27;1990-01-01 05:00:00&#x27;,\n",
       "               &#x27;1990-01-01 06:00:00&#x27;, &#x27;1990-01-01 07:00:00&#x27;,\n",
       "               &#x27;1990-01-01 08:00:00&#x27;, &#x27;1990-01-01 09:00:00&#x27;,\n",
       "               ...\n",
       "               &#x27;1990-03-31 14:00:00&#x27;, &#x27;1990-03-31 15:00:00&#x27;,\n",
       "               &#x27;1990-03-31 16:00:00&#x27;, &#x27;1990-03-31 17:00:00&#x27;,\n",
       "               &#x27;1990-03-31 18:00:00&#x27;, &#x27;1990-03-31 19:00:00&#x27;,\n",
       "               &#x27;1990-03-31 20:00:00&#x27;, &#x27;1990-03-31 21:00:00&#x27;,\n",
       "               &#x27;1990-03-31 22:00:00&#x27;, &#x27;1990-03-31 23:00:00&#x27;],\n",
       "              dtype=&#x27;datetime64[ns]&#x27;, name=&#x27;valid_time&#x27;, length=2160, freq=None))</pre></div></li><li class='xr-var-item'><div class='xr-index-name'><div>latitude</div></div><div class='xr-index-preview'>PandasIndex</div><div></div><input id='index-30726b0d-1fe1-489d-aa2b-4fa0780097fd' class='xr-index-data-in' type='checkbox'/><label for='index-30726b0d-1fe1-489d-aa2b-4fa0780097fd' title='Show/Hide index repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-index-data'><pre>PandasIndex(Index([  90.0,  89.75,   89.5,  89.25,   89.0,  88.75,   88.5,  88.25,   88.0,\n",
       "        87.75,\n",
       "       ...\n",
       "       -87.75,  -88.0, -88.25,  -88.5, -88.75,  -89.0, -89.25,  -89.5, -89.75,\n",
       "        -90.0],\n",
       "      dtype=&#x27;float64&#x27;, name=&#x27;latitude&#x27;, length=721))</pre></div></li><li class='xr-var-item'><div class='xr-index-name'><div>longitude</div></div><div class='xr-index-preview'>PandasIndex</div><div></div><input id='index-f6c899a5-d378-4a83-9c78-51a6bcc81191' class='xr-index-data-in' type='checkbox'/><label for='index-f6c899a5-d378-4a83-9c78-51a6bcc81191' title='Show/Hide index repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-index-data'><pre>PandasIndex(Index([   0.0,   0.25,    0.5,   0.75,    1.0,   1.25,    1.5,   1.75,    2.0,\n",
       "         2.25,\n",
       "       ...\n",
       "        357.5, 357.75,  358.0, 358.25,  358.5, 358.75,  359.0, 359.25,  359.5,\n",
       "       359.75],\n",
       "      dtype=&#x27;float64&#x27;, name=&#x27;longitude&#x27;, length=1440))</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-bde59bbb-f636-4df6-8be0-5bfd00eedac6' class='xr-section-summary-in' type='checkbox'  checked><label for='section-bde59bbb-f636-4df6-8be0-5bfd00eedac6' class='xr-section-summary' >Attributes: <span>(6)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><dl class='xr-attrs'><dt><span>GRIB_centre :</span></dt><dd>ecmf</dd><dt><span>GRIB_centreDescription :</span></dt><dd>European Centre for Medium-Range Weather Forecasts</dd><dt><span>GRIB_subCentre :</span></dt><dd>0</dd><dt><span>Conventions :</span></dt><dd>CF-1.7</dd><dt><span>institution :</span></dt><dd>European Centre for Medium-Range Weather Forecasts</dd><dt><span>history :</span></dt><dd>2024-11-22T11:17 GRIB to CDM+CF via cfgrib-0.9.14.1/ecCodes-2.36.0 with {&quot;source&quot;: &quot;data.grib&quot;, &quot;filter_by_keys&quot;: {&quot;stream&quot;: [&quot;oper&quot;]}, &quot;encode_cf&quot;: [&quot;parameter&quot;, &quot;time&quot;, &quot;geography&quot;, &quot;vertical&quot;]}</dd></dl></div></li></ul></div></div>"
      ],
      "text/plain": [
       "<xarray.Dataset> Size: 9GB\n",
       "Dimensions:     (valid_time: 2160, latitude: 721, longitude: 1440)\n",
       "Coordinates:\n",
       "    number      int64 8B 0\n",
       "  * valid_time  (valid_time) datetime64[ns] 17kB 1990-01-01 ... 1990-03-31T23...\n",
       "  * latitude    (latitude) float64 6kB 90.0 89.75 89.5 ... -89.5 -89.75 -90.0\n",
       "  * longitude   (longitude) float64 12kB 0.0 0.25 0.5 0.75 ... 359.2 359.5 359.8\n",
       "    expver      (valid_time) <U4 35kB '0001' '0001' '0001' ... '0001' '0001'\n",
       "Data variables:\n",
       "    msshf       (valid_time, latitude, longitude) float32 9GB 4.759 ... 22.54\n",
       "Attributes:\n",
       "    GRIB_centre:             ecmf\n",
       "    GRIB_centreDescription:  European Centre for Medium-Range Weather Forecasts\n",
       "    GRIB_subCentre:          0\n",
       "    Conventions:             CF-1.7\n",
       "    institution:             European Centre for Medium-Range Weather Forecasts\n",
       "    history:                 2024-11-22T11:17 GRIB to CDM+CF via cfgrib-0.9.1..."
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"if __name__ == '__main__':\\n    variable = sys.argv[1]\\n    year = sys.argv[2]\\n    level = int(sys.argv[3])\\n\\n    if not all_csv_files_exist(variable, year, level):\\n        process_data(variable, year, level)\\n    else:\\n        print(f'All CSV files for variable: {variable}, year: {year}, and level: {level} already exist. Skipping processing.')\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "from datetime import datetime\n",
    "#from guppy import hpy\n",
    "import os\n",
    "\n",
    "dataset_path = '/work/FAC/FGSE/IDYST/tbeucler/default/fabien/repos/cleaner_version/data/datasets_1h'\n",
    "track_path = '/work/FAC/FGSE/IDYST/tbeucler/default/fabien/repos/cleaner_version/pre_processing/tracks/ALL_TRACKS/tracks_1h'\n",
    "\n",
    "# Créer une instance de heapy\n",
    "#hp = hpy()\n",
    "# DOESN'T WORK 10m_v_component_of_wind, 2m_temperature, 10m_u_component_of_wind, 2m_dewpoint_temperature for 2020: too large\n",
    "# Define a function to open datasets and concatenate them\n",
    "def open_and_concatenate(year, variable, months, way, level=0):\n",
    "    datasets = [xr.open_dataset(f'{way}{variable}/ERA5_{year}-{month}_{variable}.nc') for month in months]\n",
    "    if variable == 'geopotential' and level != 0:\n",
    "        datasets = [dataset.sel(level=level) for dataset in datasets]\n",
    "    keys = list(datasets[0].coords.keys())\n",
    "    if keys == ['number', 'valid_time', 'latitude', 'longitude', 'expver']:\n",
    "        dim = 'valid_time'\n",
    "    else:\n",
    "        dim = 'time'\n",
    "    return xr.concat(datasets, dim=dim), dim\n",
    "\n",
    "# Define a function to calculate statistics\n",
    "def calculate_statistics(data_array):\n",
    "    return {\n",
    "        'mean': np.mean(data_array),\n",
    "        'min': np.min(data_array),\n",
    "        'max': np.max(data_array),\n",
    "        'std': np.std(data_array),\n",
    "    }\n",
    "\n",
    "# Function to log processing details\n",
    "def log_processing(variable, year, level, storm_number):\n",
    "    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    log_message = f'Processed variable: {variable}, Year: {year}, Level: {level}, Timestamp: {timestamp}, Storm number:{storm_number}'\n",
    "    with open(f'{dataset_path}/processing_log.txt', 'a') as log_file:\n",
    "        log_file.write(log_message + '\\n')\n",
    "\n",
    "# Créer une instance de heapy\n",
    "#hp = hpy()\n",
    "\n",
    "# Function to check if all CSV files exist\n",
    "def all_csv_files_exist(variable, year, level):\n",
    "    directory = f'{dataset_path}/{variable}'\n",
    "    if not os.path.exists(directory):\n",
    "        return False\n",
    "\n",
    "    for storm_dir in os.listdir(directory):\n",
    "        storm_path = os.path.join(directory, storm_dir)\n",
    "        if os.path.isdir(storm_path):\n",
    "            for stat in ['mean', 'min', 'max', 'std']:\n",
    "                file_path = os.path.join(storm_path, f'{stat}_{storm_dir.split(\"_\")[1]}_{level}.csv')\n",
    "                if not os.path.exists(file_path):\n",
    "                    return False\n",
    "    return True\n",
    "\n",
    "variable = 'mean_surface_sensible_heat_flux'#sys.argv[1]\n",
    "year = 1991#sys.argv[2]\n",
    "level = 0#int(sys.argv[3])\n",
    "\n",
    "# Main function to process data\n",
    "#def process_data(variable, year, level=0):\n",
    "year = int(year)\n",
    "year_next = year + 1\n",
    "month_act = [10, 11, 12]\n",
    "month_next = [1, 2, 3]\n",
    "if variable == 'geopotential':\n",
    "    way = '/work/FAC/FGSE/IDYST/tbeucler/default/raw_data/ECMWF/ERA5_hourly_PL/'\n",
    "else:\n",
    "    way = '/work/FAC/FGSE/IDYST/tbeucler/default/raw_data/ECMWF/ERA5/SL/'\n",
    "\n",
    "# Open and concatenate datasets\n",
    "if year == 1990:\n",
    "    dataset_act, dim = open_and_concatenate(str(year), variable, month_next, way, level)\n",
    "    dataset_next, dim = open_and_concatenate(str(year_next), variable, month_next, way, level)\n",
    "    dataset = xr.concat([dataset_act, dataset_next], dim=dim)\n",
    "    dataset = dataset.chunk({dim: 10})\n",
    "elif year == 2021:\n",
    "    dataset, dim = open_and_concatenate(str(year), variable, month_next, way, level)\n",
    "else:\n",
    "    dataset_act, dim = open_and_concatenate(str(year), variable, month_act, way, level)\n",
    "    dataset_next, dim = open_and_concatenate(str(year_next), variable, month_next, way, level)\n",
    "    dataset = xr.concat([dataset_act, dataset_next], dim=dim)\n",
    "    dataset = dataset.chunk({dim: 10})\n",
    "\n",
    "# Determine the specific variable to extract\n",
    "specific_var = next(var for var in dataset.variables if var not in ['longitude', 'latitude', 'time', 'level','number', 'valid_time', 'expver'])\n",
    "\n",
    "# Import all tracks and convert dates\n",
    "dates = pd.read_csv(f'/work/FAC/FGSE/IDYST/tbeucler/default/fabien/repos/cleaner_version/pre_processing/tracks/storm_dates.csv', \n",
    "                parse_dates=['start_date', 'end_date'])\n",
    "dates['year'] = dates['start_date'].dt.year\n",
    "try:\n",
    "    dates.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Find the indices for storms within the specified timeframe\n",
    "if year == 1990:\n",
    "    index_start_october = dates[(dates['start_date'].dt.month <= 3) & (dates['start_date'].dt.year == year)].index[0]\n",
    "    index_end_march = dates[(dates['end_date'].dt.month <= 3) & (dates['end_date'].dt.year == year_next)].index[0]\n",
    "elif year == 2021:\n",
    "    index_start_october = dates[(dates['start_date'].dt.month <= 3) & (dates['start_date'].dt.year == year)].index[0]\n",
    "    index_end_march = dates[(dates['end_date'].dt.year == 2021)].index[0]\n",
    "else:\n",
    "# Chercher start_october dans year, sinon chercher dès janvier de year_next\n",
    "    index_start_october = dates[((dates['start_date'].dt.month >= 10) & (dates['start_date'].dt.year == year)) | ((dates['start_date'].dt.year == year_next) & (dates['start_date'].dt.month >= 1))].index[0]\n",
    "    index_end_march_first = dates[((dates['end_date'].dt.month <= 3) & (dates['end_date'].dt.year == year_next))].index\n",
    "    #print(index_start_october, index_end_march_first, '3rd condition start_october + index_end_march_first')\n",
    "    if len(index_end_march_first) > 0:\n",
    "        index_end_march = index_end_march_first[-1]\n",
    "        #print(index_end_march, 'index_end_march 1st condition of 2nd condition')\n",
    "    else:\n",
    "        # Si year_next ne renvoie rien, chercher la dernière instance de tempête dans year\n",
    "        index_end_march = dates[((dates['end_date'].dt.year == year) & (dates['end_date'].dt.month <= 12))].index[-1]\n",
    "        #print(index_end_march, 'index_end_march 2nd condition of 2nd condition')\n",
    "# Process each storm\n",
    "for i in range(index_start_october, index_end_march + 1):\n",
    "    storm_number = dates.at[i, 'storm_index']\n",
    "    track = pd.read_csv(f'{track_path}/storm_{storm_number}.csv')\n",
    "    start_date = dates.at[i, 'start_date']\n",
    "    end_date = dates.at[i, 'end_date']\n",
    "    if dim == 'valid_time':\n",
    "        storm_data = dataset[specific_var].sel(valid_time=slice(start_date, end_date))\n",
    "    else:\n",
    "        storm_data = dataset[specific_var].sel(time=slice(start_date, end_date))\n",
    "\n",
    "    # Initialize lists to store statistics\n",
    "    stats = {'mean': [], 'min': [], 'max': [], 'std': []}\n",
    "    #, 'skewness': [], 'kurtosis': []\n",
    "\n",
    "    # Process each time step\n",
    "    if dim == 'valid_time':\n",
    "        time = storm_data.valid_time\n",
    "    else:\n",
    "        time = storm_data.time\n",
    "    for t_index in range(0, len(time)):#, time_step in enumerate(storm_data.time):\n",
    "        #data_slice = storm_data.sel(time=time_step).values\n",
    "\n",
    "        # Extract coordinates for the current time step\n",
    "\n",
    "        lon_e_temp = track.loc[t_index, 'lon_east']\n",
    "        lon_w_temp = track.loc[t_index, 'lon_west']\n",
    "        lat_s_temp = track.loc[t_index, 'lat_south']\n",
    "        lat_n_temp = track.loc[t_index, 'lat_north']\n",
    "\n",
    "        lon_test = np.asanyarray(storm_data.longitude[:])\n",
    "        lat_test = np.asanyarray(storm_data.latitude[:])\n",
    "\n",
    "        closest_lon_w = np.abs(lon_test - lon_w_temp).argmin()\n",
    "        closest_lon_e = np.abs(lon_test - lon_e_temp).argmin()\n",
    "        closest_lat_s = np.abs(lat_test - lat_s_temp).argmin()\n",
    "        closest_lat_n = np.abs(lat_test - lat_n_temp).argmin()\n",
    "\n",
    "        closest_lon_w_coor = lon_test[closest_lon_w]\n",
    "        closest_lon_e_coor = lon_test[closest_lon_e]\n",
    "        closest_lat_s_coor = lat_test[closest_lat_s]\n",
    "        closest_lat_n_coor = lat_test[closest_lat_n]\n",
    "\n",
    "        # Use .roll to handle the 0°/360° boundary\n",
    "        if closest_lon_w_coor > 100 and closest_lon_e_coor < 100:\n",
    "            roll_shift = {'longitude': int(round(closest_lon_e_coor, 0)), 'longitude': int(round(closest_lon_w_coor, 0))}\n",
    "            storm_data_rolled = storm_data.roll(roll_shift, roll_coords=True)\n",
    "        else:\n",
    "            storm_data_rolled = storm_data\n",
    "\n",
    "        # Slice the dataset based on the rolled longitudes and latitudes\n",
    "        if dim == 'valid_time':\n",
    "            temp_ds_time = storm_data_rolled.isel(valid_time=t_index)\n",
    "        else:\n",
    "            temp_ds_time = storm_data_rolled.isel(time=t_index)\n",
    "        #[specific_var].isel(time=t_index)\n",
    "        temp_ds = temp_ds_time.sel(latitude=slice(closest_lat_n_coor, closest_lat_s_coor),\n",
    "                                    longitude=slice(closest_lon_w_coor, closest_lon_e_coor)).values\n",
    "\n",
    "        # Calculate statistics for the sliced data\n",
    "        step_stats = calculate_statistics(temp_ds)\n",
    "        for key in stats:\n",
    "            stats[key].append(step_stats[key])\n",
    "\n",
    "    # Save statistics to CSV files\n",
    "    for key in stats:\n",
    "        directory = f'{dataset_path}/{variable}/storm_{storm_number}'\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "        pd.DataFrame(stats[key]).to_csv(f'{directory}/{key}_{storm_number}_{level}.csv')\n",
    "\n",
    "    # Log the processing details\n",
    "    log_processing(variable, year, level, storm_number)\n",
    "\n",
    "'''if __name__ == '__main__':\n",
    "    variable = 'mean_surface_sensible_heat_flux'#sys.argv[1]\n",
    "    year = 1991#sys.argv[2]\n",
    "    level = 0#int(sys.argv[3])\n",
    "    process_data(variable, year, level)\n",
    "'''\n",
    "'''if __name__ == '__main__':\n",
    "    variable = sys.argv[1]\n",
    "    year = sys.argv[2]\n",
    "    level = int(sys.argv[3])\n",
    "\n",
    "    if not all_csv_files_exist(variable, year, level):\n",
    "        process_data(variable, year, level)\n",
    "    else:\n",
    "        print(f'All CSV files for variable: {variable}, year: {year}, and level: {level} already exist. Skipping processing.')'''\n",
    "\n",
    "# Obtenir un instantané de l'utilisation de la mémoire\n",
    "#h = hp.heap()\n",
    "\n",
    "# Imprimer l'information d'utilisation de la mémoire\n",
    "#print(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_ds = temp_ds_time.sel(latitude=slice(closest_lat_n_coor, closest_lat_s_coor),\n",
    "                           longitude=slice(closest_lon_w_coor, closest_lon_e_coor)).values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "closest_lon_w_coor = 3\n",
    "closest_lon_e_coor = 355\n",
    "\n",
    "# Use .roll to handle the 0°/360° boundary\n",
    "if closest_lon_w_coor > 100 and closest_lon_e_coor < 100:\n",
    "    roll_shift = {'longitude': int(round(closest_lon_e_coor, 0)), 'longitude': int(round(closest_lon_w_coor, 0))}\n",
    "    storm_data_rolled = storm_data.roll(roll_shift, roll_coords=True)\n",
    "else:\n",
    "    storm_data_rolled = storm_data\n",
    "\n",
    "# Slice the dataset based on the rolled longitudes and latitudes\n",
    "if dim == 'valid_time':\n",
    "    temp_ds_time = storm_data_rolled.isel(valid_time=t_index)\n",
    "else:\n",
    "    temp_ds_time = storm_data_rolled.isel(time=t_index)\n",
    "#[specific_var].isel(time=t_index)\n",
    "temp_ds = temp_ds_time.sel(latitude=slice(closest_lat_n_coor, closest_lat_s_coor),\n",
    "                            longitude=slice(closest_lon_w_coor, closest_lon_e_coor)).values\n",
    "\n",
    "# Calculate statistics for the sliced data\n",
    "step_stats = calculate_statistics(temp_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    old_dataset = xr.open_dataset('/work/FAC/FGSE/IDYST/tbeucler/default/raw_data/ECMWF/ERA5/SL/instantaneous_10m_wind_gust/ERA5_1990-1_instantaneous_10m_wind_gust.nc')\n",
    "new_dataset = xr.open_dataset('/work/FAC/FGSE/IDYST/tbeucler/default/raw_data/ECMWF/ERA5/SL/mean_surface_sensible_heat_flux/ERA5_1990-1_mean_surface_sensible_heat_flux.nc')\n",
    "specific_var = next(var for var in new_dataset.variables if var not in ['longitude', 'latitude', 'time', 'level','number', 'valid_time', 'expver'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "operating_system ='curnagl'\n",
    "if operating_system == 'win':\n",
    "    path = f'C:/Users/fabau/OneDrive/Documents/GitHub/master-project/'\n",
    "elif operating_system == 'curnagl':\n",
    "    path = f'/work/FAC/FGSE/IDYST/tbeucler/default/fabien/repos/'\n",
    "else:\n",
    "    path = f'/Users/fabienaugsburger/Documents/GitHub/master-project-cleaned/'\n",
    "\n",
    "# Add the path to your custom library\n",
    "if operating_system == 'curnagl':\n",
    "    custom_library_path = sys.path.append(f'{path}/cleaner_version/util/processing/')\n",
    "\n",
    "import dataset_creation_1h\n",
    "\n",
    "variable = 'mean_surface_sensible_heat_flux'#sys.argv[1]\n",
    "year = 1992#sys.argv[2]\n",
    "level = 0#int(sys.argv[3])\n",
    "dataset_creation_1h.process_data(variable, year, level)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
